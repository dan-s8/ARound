{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, randint\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "file_path = '/Users/zmy/Documents/A Round Ent/cleaned_data_allpop100k.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    '# Shows',\n",
    "    'Avg. Tickets Sold',\n",
    "    'Avg. Event Capacity',\n",
    "    'Avg. Capacity Sold',\n",
    "    'Ticket Price Min',\n",
    "    'Ticket Price Max',\n",
    "    'Ticket Price Avg. USD',\n",
    "    'sp followers',\n",
    "    'sp popularity',\n",
    "    'yt View Count',\n",
    "    'yt Subscriber Count',\n",
    "    'yt Video Count',\n",
    "    'Month',\n",
    "    'is_holiday',\n",
    "    'day_of_week_1',\n",
    "    'day_of_week_2',\n",
    "    'day_of_week_3',\n",
    "    'day_of_week_4',\n",
    "    'day_of_week_5',\n",
    "    'day_of_week_6'\n",
    "]\n",
    "target = 'Avg. Gross USD'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected = data[['Event Date'] + features + [target]].dropna()\n",
    "data_selected['Event Date'] = pd.to_datetime(data_selected['Event Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    '# Shows',\n",
    "    'Avg. Tickets Sold',\n",
    "    'Avg. Event Capacity',\n",
    "    'Avg. Capacity Sold',\n",
    "    'Ticket Price Min',\n",
    "    'Ticket Price Max',\n",
    "    'Ticket Price Avg. USD',\n",
    "    'sp followers',\n",
    "    'sp popularity',\n",
    "    'yt View Count',\n",
    "    'yt Subscriber Count',\n",
    "    'yt Video Count',\n",
    "]\n",
    "\n",
    "for column in numeric_features:\n",
    "    data_selected[column] = data_selected[column].astype(str).str.replace(',', '').str.replace('%', '').astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected['is_holiday'] = data_selected['is_holiday'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
      "/var/folders/2y/szk7msh55b58ppjffm1bdsfw0000gn/T/ipykernel_15689/3779859810.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n"
     ]
    }
   ],
   "source": [
    "# Create interaction terms more efficiently\n",
    "interaction_terms = pd.DataFrame(index=data_selected.index)\n",
    "for feature1 in numeric_features:\n",
    "    for feature2 in numeric_features:\n",
    "        if feature1 != feature2:\n",
    "            interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
    "\n",
    "data_selected_interactions = pd.concat([data_selected, interaction_terms], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between consecutive rows\n",
    "data_selected_diff = data_selected_interactions.diff().dropna()\n",
    "\n",
    "# Ensure all feature names are strings\n",
    "data_selected_diff.columns = data_selected_diff.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature matrix X and the target vector y\n",
    "X = data_selected_diff[features]\n",
    "y = data_selected_diff[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with PolynomialFeatures, StandardScaler, and GradientBoostingRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gb', GradientBoostingRegressor(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to try for Gradient Boosting\n",
    "param_dist = {\n",
    "    'gb__learning_rate': uniform(0.01, 0.3),\n",
    "    'gb__max_depth': randint(3, 10),\n",
    "    'gb__n_estimators': randint(100, 500),\n",
    "    'gb__subsample': uniform(0.6, 0.4),\n",
    "    'gb__min_samples_split': randint(2, 10),\n",
    "    'gb__min_samples_leaf': randint(1, 4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'gb__learning_rate': 0.05680559213273095, 'gb__max_depth': 5, 'gb__min_samples_leaf': 3, 'gb__min_samples_split': 4, 'gb__n_estimators': 187, 'gb__subsample': 0.7334834444556088}\n",
      "Best cross-validation score: 1612128017.6303527\n"
     ]
    }
   ],
   "source": [
    "# Perform randomized search to find the best parameters\n",
    "random_search = RandomizedSearchCV(pipeline, param_dist, n_iter=20, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {-random_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model for predictions\n",
    "best_gradient_boosting = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 1203110550.3520772\n",
      "Testing MSE: 1642487723.1109018\n",
      "Training RMSE: 34685.884021487436\n",
      "Testing RMSE: 40527.616795352056\n",
      "Training R²: 0.9475540142386237\n",
      "Testing R²: 0.9316882773728393\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model\n",
    "y_train_pred = best_gradient_boosting.predict(X_train)\n",
    "y_test_pred = best_gradient_boosting.predict(X_test)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Testing MSE: {test_mse}\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Testing RMSE: {test_rmse}\")\n",
    "print(f\"Training R²: {train_r2}\")\n",
    "print(f\"Testing R²: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate polynomial features for the full dataset\n",
    "data_poly = best_gradient_boosting.named_steps['poly'].transform(data_selected_diff[features])\n",
    "data_poly_scaled = best_gradient_boosting.named_steps['scaler'].transform(data_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 5 Predicted Avg. Gross USD values:\n",
      "[ 49266.35818514  54307.57418685 -56474.53408315 258575.94320344\n",
      "  76265.2298096 ]\n"
     ]
    }
   ],
   "source": [
    "# Predict using the best Gradient Boosting model\n",
    "predicted_gross_revenue = best_gradient_boosting.named_steps['gb'].predict(data_poly_scaled)\n",
    "\n",
    "print(\"Last 5 Predicted Avg. Gross USD values:\")\n",
    "print(predicted_gross_revenue[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline, including model, polynomial features, and scaler, saved.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Save the model, polynomial features, and scaler\n",
    "joblib.dump(pipeline, 'best_gradient_boosting_pipeline.pkl')\n",
    "print(\"Pipeline, including model, polynomial features, and scaler, saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model without Ticket Sold, without Capacity Sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'gb__learning_rate': 0.0646708263364187, 'gb__max_depth': 6, 'gb__min_samples_leaf': 2, 'gb__min_samples_split': 7, 'gb__n_estimators': 489, 'gb__subsample': 0.6831766651472755}\n",
      "Best cross-validation score: 3502790826.356483\n",
      "Training MSE: 3111302561.9198465\n",
      "Testing MSE: 3976623167.681465\n",
      "Training RMSE: 55779.05128199875\n",
      "Testing RMSE: 63060.47230778933\n",
      "Training R²: 0.864372122899238\n",
      "Testing R²: 0.8346106488340216\n",
      "Last 5 Predicted Avg. Gross USD values:\n",
      "[ 31785.07966772 116700.86896071 -54148.25914986 219715.34865751\n",
      "  65314.76003746]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "file_path = '/Users/zmy/Documents/A Round Ent/cleaned_data_allpop100k.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define the features and target, excluding 'Avg. Capacity Sold' and 'Avg. Tickets Sold'\n",
    "features = [\n",
    "    '# Shows',\n",
    "    'Avg. Event Capacity',\n",
    "    'Ticket Price Min',\n",
    "    'Ticket Price Max',\n",
    "    'Ticket Price Avg. USD',\n",
    "    'sp followers',\n",
    "    'sp popularity',\n",
    "    'yt View Count',\n",
    "    'yt Subscriber Count',\n",
    "    'yt Video Count',\n",
    "    'Month',\n",
    "    'is_holiday',\n",
    "    'day_of_week_1',\n",
    "    'day_of_week_2',\n",
    "    'day_of_week_3',\n",
    "    'day_of_week_4',\n",
    "    'day_of_week_5',\n",
    "    'day_of_week_6'\n",
    "]\n",
    "target = 'Avg. Gross USD'\n",
    "\n",
    "# Select and preprocess the data\n",
    "data_selected = data[['Event Date'] + features + [target]].dropna()\n",
    "data_selected['Event Date'] = pd.to_datetime(data_selected['Event Date'])\n",
    "\n",
    "# Convert numeric features to float\n",
    "numeric_features = [\n",
    "    '# Shows',\n",
    "    'Avg. Event Capacity',\n",
    "    'Ticket Price Min',\n",
    "    'Ticket Price Max',\n",
    "    'Ticket Price Avg. USD',\n",
    "    'sp followers',\n",
    "    'sp popularity',\n",
    "    'yt View Count',\n",
    "    'yt Subscriber Count',\n",
    "    'yt Video Count',\n",
    "]\n",
    "\n",
    "for column in numeric_features:\n",
    "    data_selected[column] = data_selected[column].astype(str).str.replace(',', '').str.replace('%', '').astype(float)\n",
    "\n",
    "data_selected['is_holiday'] = data_selected['is_holiday'].astype(int)\n",
    "\n",
    "# Create interaction terms more efficiently\n",
    "interaction_terms = pd.DataFrame(index=data_selected.index)\n",
    "for feature1 in numeric_features:\n",
    "    for feature2 in numeric_features:\n",
    "        if feature1 != feature2:\n",
    "            interaction_terms[f'{feature1} * {feature2}'] = data_selected[feature1] * data_selected[feature2]\n",
    "\n",
    "data_selected_interactions = pd.concat([data_selected, interaction_terms], axis=1)\n",
    "\n",
    "# Calculate the difference between consecutive rows\n",
    "data_selected_diff = data_selected_interactions.diff().dropna()\n",
    "\n",
    "# Ensure all feature names are strings\n",
    "data_selected_diff.columns = data_selected_diff.columns.astype(str)\n",
    "\n",
    "# Define the feature matrix X and the target vector y\n",
    "X = data_selected_diff[features]\n",
    "y = data_selected_diff[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, test_size=0.8, random_state=42)\n",
    "\n",
    "# Create a pipeline with PolynomialFeatures, StandardScaler, and GradientBoostingRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gb', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameters to try for Gradient Boosting\n",
    "param_dist = {\n",
    "    'gb__learning_rate': uniform(0.01, 0.3),\n",
    "    'gb__max_depth': randint(3, 10),\n",
    "    'gb__n_estimators': randint(100, 500),\n",
    "    'gb__subsample': uniform(0.6, 0.4),\n",
    "    'gb__min_samples_split': randint(2, 10),\n",
    "    'gb__min_samples_leaf': randint(1, 4)\n",
    "}\n",
    "\n",
    "# Perform randomized search to find the best parameters\n",
    "random_search = RandomizedSearchCV(pipeline, param_dist, n_iter=20, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {-random_search.best_score_}\")\n",
    "\n",
    "# Use the best model for predictions\n",
    "best_gradient_boosting = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "y_train_pred = best_gradient_boosting.predict(X_train)\n",
    "y_test_pred = best_gradient_boosting.predict(X_test)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Testing MSE: {test_mse}\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Testing RMSE: {test_rmse}\")\n",
    "print(f\"Training R²: {train_r2}\")\n",
    "print(f\"Testing R²: {test_r2}\")\n",
    "\n",
    "# Generate polynomial features for the full dataset\n",
    "data_poly = best_gradient_boosting.named_steps['poly'].transform(data_selected_diff[features])\n",
    "data_poly_scaled = best_gradient_boosting.named_steps['scaler'].transform(data_poly)\n",
    "\n",
    "# Predict using the best Gradient Boosting model\n",
    "predicted_gross_revenue = best_gradient_boosting.named_steps['gb'].predict(data_poly_scaled)\n",
    "\n",
    "print(\"Last 5 Predicted Avg. Gross USD values:\")\n",
    "print(predicted_gross_revenue[-5:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IU Concert Avg. Revenue & Gross Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Avg. Gross USD for IU's concerts:\n",
      "457574.1079572082\n",
      "Predicted Avg. Gross USD per concert: 457574.1079572082\n",
      "Total Predicted Revenue for 4 concerts: 1830296.4318288327\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# IU's concert information\n",
    "iu_concerts = {\n",
    "    '# Shows': [4],\n",
    "    'Avg. Event Capacity': [18890],\n",
    "    'Ticket Price Min': [59],\n",
    "    'Ticket Price Max': [219.5],\n",
    "    'Ticket Price Avg. USD': [139.25],\n",
    "    'sp followers': [5675418],\n",
    "    'sp popularity': [0],  # Assuming this needs to be provided\n",
    "    'yt View Count': [2992142616],\n",
    "    'yt Subscriber Count': [9720000],\n",
    "    'yt Video Count': [295],\n",
    "    'Month': [7],  # Assuming these concerts are in July\n",
    "    'is_holiday': [0],  # Assuming it's not a holiday\n",
    "    'day_of_week_1': [1],  # Assuming the concert is on a Monday (one-hot encoded)\n",
    "    'day_of_week_2': [1],  # Assuming the concert is on a Tuesday (one-hot encoded)\n",
    "    'day_of_week_3': [0],  # Assuming the concert is not on a Wednesday\n",
    "    'day_of_week_4': [1],  # Assuming the concert is on a Thursday (one-hot encoded)\n",
    "    'day_of_week_5': [1],  # Assuming the concert is on a Friday (one-hot encoded)\n",
    "    'day_of_week_6': [0],  # Assuming the concert is not on a Saturday\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "iu_df = pd.DataFrame(iu_concerts)\n",
    "\n",
    "# Apply polynomial features and scaling\n",
    "iu_poly = best_gradient_boosting.named_steps['poly'].transform(iu_df)\n",
    "iu_poly_scaled = best_gradient_boosting.named_steps['scaler'].transform(iu_poly)\n",
    "\n",
    "# Predict the sales revenue\n",
    "iu_predicted_revenue = best_gradient_boosting.named_steps['gb'].predict(iu_poly_scaled)\n",
    "\n",
    "print(\"Predicted Avg. Gross USD for IU's concerts:\")\n",
    "print(iu_predicted_revenue[0])\n",
    "\n",
    "# Predicted Avg. Gross USD for IU's concerts\n",
    "avg_gross_usd = iu_predicted_revenue[0]\n",
    "\n",
    "# Total revenue calculation\n",
    "number_of_shows = iu_concerts['# Shows'][0]\n",
    "total_revenue = avg_gross_usd * number_of_shows\n",
    "\n",
    "print(f\"Predicted Avg. Gross USD per concert: {avg_gross_usd}\")\n",
    "print(f\"Total Predicted Revenue for {number_of_shows} concerts: {total_revenue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 3111302561.9198465\n",
      "Testing MSE: 3976623167.681465\n",
      "Training RMSE: 55779.05128199875\n",
      "Testing RMSE: 63060.47230778933\n",
      "Training R²: 0.864372122899238\n",
      "Testing R²: 0.8346106488340216\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Evaluate the best model\n",
    "y_train_pred = best_gradient_boosting.predict(X_train)\n",
    "y_test_pred = best_gradient_boosting.predict(X_test)\n",
    "\n",
    "# Calculate MSE and RMSE for training and testing sets\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "# Calculate R² scores\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Testing MSE: {test_mse}\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Testing RMSE: {test_rmse}\")\n",
    "print(f\"Training R²: {train_r2}\")\n",
    "print(f\"Testing R²: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Tickets Sold & Capacity\n",
    "Assuming popularity is 80\n",
    "Avg ticket price is 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date              city  Predicted Avg. Gross USD\n",
      "0  2024-07-22  Washington, D.C.             525084.846262\n",
      "1  2024-07-25      Rosemont, IL             525084.846262\n",
      "2  2024-07-30       Oakland, CA             524004.713580\n",
      "3  2024-08-02     Inglewood, CA             525084.846262\n",
      "Total Predicted Gross Revenue for four concerts: $2,099,259.25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "# Load the entire pipeline\n",
    "loaded_pipeline = joblib.load('best_gradient_boosting_pipeline.pkl')\n",
    "\n",
    "# Concert details\n",
    "concerts = [\n",
    "    {\"date\": \"2024-07-22\", \"day_of_week\": \"Monday\", \"capacity\": 20356, \"city\": \"Washington, D.C.\"},\n",
    "    {\"date\": \"2024-07-25\", \"day_of_week\": \"Thursday\", \"capacity\": 18500, \"city\": \"Rosemont, IL\"},\n",
    "    {\"date\": \"2024-07-30\", \"day_of_week\": \"Tuesday\", \"capacity\": 19200, \"city\": \"Oakland, CA\"},\n",
    "    {\"date\": \"2024-08-02\", \"day_of_week\": \"Friday\", \"capacity\": 17505, \"city\": \"Inglewood, CA\"}\n",
    "]\n",
    "\n",
    "# Features\n",
    "avg_event_capacity = 18890\n",
    "ticket_price_min = 59\n",
    "ticket_price_max = 219.5\n",
    "ticket_price_avg_usd = 200\n",
    "yt_view_count = 2992142616\n",
    "yt_subscriber_count = 9720000\n",
    "yt_video_count = 295\n",
    "sp_followers = 5675418\n",
    "sp_popularity = 80  # Assuming popularity score\n",
    "num_shows = 4\n",
    "\n",
    "# Prepare data for prediction\n",
    "data_for_prediction = pd.DataFrame(concerts)\n",
    "data_for_prediction['Avg. Event Capacity'] = avg_event_capacity\n",
    "data_for_prediction['Avg. Capacity Sold'] = data_for_prediction['capacity']  # Assuming full capacity sold\n",
    "data_for_prediction['Avg. Tickets Sold'] = data_for_prediction['Avg. Capacity Sold']\n",
    "data_for_prediction['Ticket Price Min'] = ticket_price_min\n",
    "data_for_prediction['Ticket Price Max'] = ticket_price_max\n",
    "data_for_prediction['Ticket Price Avg. USD'] = ticket_price_avg_usd\n",
    "data_for_prediction['yt View Count'] = yt_view_count\n",
    "data_for_prediction['yt Subscriber Count'] = yt_subscriber_count\n",
    "data_for_prediction['yt Video Count'] = yt_video_count\n",
    "data_for_prediction['sp followers'] = sp_followers\n",
    "data_for_prediction['sp popularity'] = sp_popularity\n",
    "data_for_prediction['# Shows'] = num_shows\n",
    "data_for_prediction['Month'] = pd.to_datetime(data_for_prediction['date']).dt.month\n",
    "data_for_prediction['is_holiday'] = 0  # Assuming no holidays for simplicity\n",
    "\n",
    "# One-hot encode days of the week\n",
    "days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "for day in days_of_week:\n",
    "    data_for_prediction[f'day_of_week_{days_of_week.index(day) + 1}'] = (data_for_prediction['day_of_week'] == day).astype(int)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data_for_prediction = data_for_prediction.drop(columns=['date', 'day_of_week', 'city'])\n",
    "\n",
    "# Ensure feature order matches the model\n",
    "features = [\n",
    "    '# Shows',\n",
    "    'Avg. Tickets Sold',\n",
    "    'Avg. Event Capacity',\n",
    "    'Avg. Capacity Sold',\n",
    "    'Ticket Price Min',\n",
    "    'Ticket Price Max',\n",
    "    'Ticket Price Avg. USD',\n",
    "    'sp followers',\n",
    "    'sp popularity',\n",
    "    'yt View Count',\n",
    "    'yt Subscriber Count',\n",
    "    'yt Video Count',\n",
    "    'Month',\n",
    "    'is_holiday',\n",
    "    'day_of_week_1',\n",
    "    'day_of_week_2',\n",
    "    'day_of_week_3',\n",
    "    'day_of_week_4',\n",
    "    'day_of_week_5',\n",
    "    'day_of_week_6'\n",
    "]\n",
    "data_for_prediction = data_for_prediction[features]\n",
    "\n",
    "# Use the loaded pipeline to make predictions\n",
    "predicted_gross_revenue = loaded_pipeline.predict(data_for_prediction)\n",
    "\n",
    "# Display results\n",
    "result_df = pd.DataFrame(concerts)\n",
    "result_df['Predicted Avg. Gross USD'] = predicted_gross_revenue\n",
    "print(result_df[['date', 'city', 'Predicted Avg. Gross USD']])\n",
    "\n",
    "# Calculate the sum of the predicted gross revenue for the four concerts\n",
    "total_predicted_revenue = predicted_gross_revenue.sum()\n",
    "print(f\"Total Predicted Gross Revenue for four concerts: ${total_predicted_revenue:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 1452242194.0099704\n",
      "Testing MSE: 1767547211.2215025\n",
      "Training RMSE: 38108.29560620588\n",
      "Testing RMSE: 42042.20749700832\n",
      "Training R²: 0.9366938695643322\n",
      "Testing R²: 0.9264870031450324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Predictions for the training and testing sets\n",
    "y_train_pred = loaded_pipeline.predict(X_train)\n",
    "y_test_pred = loaded_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate MSE and RMSE for training and testing sets\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "# Calculate R² scores\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Testing MSE: {test_mse}\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Testing RMSE: {test_rmse}\")\n",
    "print(f\"Training R²: {train_r2}\")\n",
    "print(f\"Testing R²: {test_r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
